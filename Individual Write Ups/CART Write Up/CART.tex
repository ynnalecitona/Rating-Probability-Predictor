\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{listings}

\title{ECS 189G Term Project}
\author{Leo Martinez-Perez
    \texttt{leomartinezp@ucdavis.edu}
    \and Ynna Lecitona
    \texttt{yhlecitona@ucdavis.edu}
    \and Duong Duy Nguyen
    \texttt{mdnnguyen@ucdavis.edu}
    \and Tycho Yacub
    \texttt{tsyacub@ucdavis.edu}
    \and Jessica Ma
    \texttt{jyma@ucdavis.edu}
}
\date{March 2020}

\begin{document}

\maketitle

\section{Introduction}
All quarter we've been predicting item ratings in the format of, "User i rates item j as x", where x is the predicted rating. To take it one step further, the goal of this project is to report the probabilities of a user i, rating item j from min to max rating. For example, for user 20 and movie 10, the probabilities of rating it a 1, 2, 3, ... are 0.102, 0.373, 0.111, ... respectively. We will calculate these probabilities through the use of 4 prediction methods: logit, NMF, kNN, and CART.

\section{Logit}

\section{NMF - Nonnegative Matrix Factorization}

\section{KNN - k Nearest Neighbors}

\section{CART - Classification and Regression Trees}
\subsection{Choosing a Strategy}
In order to add to our ratingProbsFit function the ability to predict the probability of each possible value of rating, we first considered two different strategies. The first was to replace the ratings with as many indicator variables as there were possible values for rating, as done in our implementation of the NMF case, run ctree for each of the indicator variables, and then have each outputted party object predict the probability of its corresponding rating, as determined by the indicator variable. The second strategy we considered, inspired by the random forests method, was to take some large number of samples from the inputted data set, run ctree on each sample, and for each row of a given test set, find the proportion of outputted party objects that predict a rating of 1, the proportion that predict a rating of 2, etc.

$\newline$
Later, we discovered that there was one more strategy we could consider, which was to call the party class's predict function with an additional argument: type="prob". Calling predict with type="prob" on a 1-row data frame named "newdata" produces an object of class "ecdf" that is also a function. If this object is assigned to a variable "ecdfFunc", it can then be called using "ecdfFunc(x)" where x is some numeric value. The value returned by this function call would then be the proportion of "similar" rows in the original training data that have ratings less than or equal to x, where "similar" rows are defined as the rows that are used to predict new cases that fall exactly in the terminal node which "newdata" would belong to. For example, if "newdata" corresponds to node 10, and node 10 in turn corresponds to a hundred rows of the original data, twenty of which have a rating less than or equal to 2, then "ecdfFunc(2)" would return 0.2. We plan to use the proportion computed by "ecdfFunc(x)" to estimate the probability that the rating is less than or equal to x.

$\newline$
The third strategy appeared to be no less accurate than our first two strategies in theory, and it also had the advantage of requiring less computation. Only a single tree would need to be generated in order to predict probabilities, while on the other hand the first strategy would require the generation of as many trees as there are possible values for rating, and the second strategy may require even more in order to accurately predict probabilities. Thus, we decided to use this last strategy to guide our implementation of the CART case.

\subsection{Using CART in ratingProbsFit}
When ratingProbsFit is run with the value of its argument predMethod equal to "CART", the return value of class recProbs contains the following information: the prediction method type ("CART"), the maximum possible rating, the mappings of user ID to mean user rating and item ID to mean item rating, and an object of class party obtained by running ctree with the data set passed in. Before running ctree, we must always transform the data first by replacing each user ID in the user ID column with the corresponding mean user rating and replacing each item ID in the item ID column with the corresponding mean item rating, for reasons described below in section 7.1. Calling ctree on the transformed data will form a recursive partitioning of the data space consisting of user mean and item mean rather than user ID and item ID. Thus, the mappings themselves are also saved in the return value, since any data we would like to predict ratings for must be transformed in the same manner before being passed to the party class's predict function.

\subsection{Predicting Rating Probabilities}
After transforming the data set "newXs", predict is called with the transformed data and the argument type set to "probs". The value returned by this call consists of a list, where each element is an empirical cumulative distribution function corresponding to one row of "newXs". The rating probabilities for each row of "newXs" can then be calculated using the following function:
\begin{lstlisting}
computeProbs <- function(ecdfFunc, maxRating) {
    probs <- sapply(1:maxRating, function(rating) 
        ecdfFunc(rating + 0.5) - ecdfFunc(rating - 0.5))
}
\end{lstlisting}
The above function "computeProbs" finds ecdf(1.5) to get the probability that the rating is a 1, ecdf(2.5) - ecdf(1.5) to get the probability that the rating is a 2 (i.e. between 1.5 and 2.5), etc.

\subsection{Results of Testing}
We tested our version of the CART prediction method for rating probabilities on the InstEval data set and the Song List data set, in each case partitioning the data set into a training set, a validation set, and a test set. For the InstEval data set, these account for 70\%, 20\%, and 10\% of the original data, respectively; for the Song List data set, they account for 80\%, 10\%, and 10\%, respectively. For each of the two data sets, we called ratingProbsFit with the training set and then called predict on the return value of the first function call with the validation set as the new data and later the test set as the new data. We then computed a comparison matrix for the validation set and one for the test set, the first column consisting of the average predicted probability that the rating is a 1, the average predicted probability that the rating is a 2, etc., and the second column corresponding to the actual proportion of ratings in the validation or test that are 1's, 2's, etc. The results for the Song List data set are below:

\begin{center}
    Validation Set
    \begin{tabular}{c|c|c}
        Rating & Avg predicted prob. & Actual proportion \\
        \hline
        1 & 0.19384487 & 0.195835 \\
        2 & 0.09295545 & 0.092725 \\
        3 & 0.14417024 & 0.143740 \\
        4 & 0.17298516 & 0.173775 \\
        5 & 0.39604427 & 0.393925 \\
    \end{tabular}
\end{center}

\begin{center}
    Test Set
    \begin{tabular}{c|c|c}
        Rating & Avg predicted prob. & Actual proportion \\
        \hline
        1 & 0.19436014 & 0.19655 \\
        2 & 0.09309556 & 0.09369 \\
        3 & 0.14442734 & 0.14330 \\
        4 & 0.17299311 & 0.17377 \\
        5 & 0.39512384 & 0.39269 \\
    \end{tabular}
\end{center}

$\newline$
In each case, the two columns are quite close - if we take each row in its percentage form, the two values are off by only a decimal point value. To get a measurement of the accuracy, we found the mean absolute prediction error (MAPE) for each comparison matrix, taking the first column to be our predicted values and the second column to be our actual values. The mean absolute prediction error turned out to be 0.001111986 for the validation data set and 0.001424474 for the test set, in each case less than a difference of 1\% in value (when the values in the matrix are expressed as percentages rather than decimal numbers).

$\newline$
We repeated the above process for the InstEval data set, computing the two comparison matrices, and found that the mean absolute prediction error was 0.002376233 for the validation data set and 0.004929582 for the test set, again in each less than a difference value of 1\% in value.

\section{Prediction Methods Comparisons}

\section{Helper Functions}
\subsection{embedDataMeans}
In certain data sets, it would be difficult to perform predictions directly. There are data sets where the item IDs are not ordinal, hence they don't have underlying ordering. The solution is to create dummy variables for these IDs until moving forward to testing them one at a time.
For example, say we're using CART as our prediction method and there is a possibility that there would be at most one data point that satisfied two conditions, this creates issues when making trees. Rather than using minsplit, we can embed the user ID and item ID to do dimension reduction. In this function, our goal was to create a new data frame, that includes the mapping between the userID and its mean ratings, as well as the itemID and its mean ratings.

\subsection{dataToMatrix}
The goal of this function is to make data handling for KNN and NMF easier. This functions turns our data frame into a data table, and creates a matrix like form of data with the userID's corresponding rating to itemID. With this function, the entries are replaced with NA if the user has not rated the item.

\subsection{ratingToDummy}


\section{Who Did What}
\subsection{Leo}

\subsection{Ynna}

\subsection{Mark}

\subsection{Tycho}

\subsection{Jessica}
Jessica contributed to the CART portion of the project, including writing the CART-specific code of ratingProbsFit and predict.recProbs, testing our version of the CART prediction method, and measuring its accuracy, i.e. computing the "comparison matrices" and finding MAPEs.

\end{document}
